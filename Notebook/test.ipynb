{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool': 'search',\n",
       "  'query': 'example query',\n",
       "  'body': 'This is a search tool request.'},\n",
       " {'tool': 'image',\n",
       "  'query': 'example image',\n",
       "  'body': 'This is an image tool request.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, os, sys\n",
    "\n",
    "def extract_tool_requests(response):\n",
    "\n",
    "    # Regex to match  the tool blocks \n",
    "    pattern = r'<tool\\$(.*?)>(.*?)</tool\\$>'\n",
    "    matches = re.findall(pattern=pattern,       # find the regex in input data \n",
    "                         string=response,       # input data \n",
    "                         flags=re.DOTALL        # flags allows the . to match newline characters as well.\n",
    "                         )\n",
    "    \n",
    "    tool_usages = []\n",
    "\n",
    "    for match in matches:\n",
    "        # print(\"match\", match)\n",
    "        attributes, body = match \n",
    "        # print(\"attributes: \", attributes)\n",
    "        # print(\"body: \", body)\n",
    "        tool_dir = {}\n",
    "\n",
    "        # Parse attributes \n",
    "        for attr in re.findall(r'(\\w+)=\"([^\"]+)\"', attributes):     # This line finds all substrings in response that match the defined pattern and return them as a list of tuples.\n",
    "            # print(\"attr1: \", attr[1])\n",
    "            tool_dir[attr[0]] = attr[1]\n",
    "            # print(\"tool_dir\", tool_dir)\n",
    "\n",
    "\n",
    "        # Add body content \n",
    "        tool_dir[\"body\"] = body.strip()\n",
    "        tool_usages.append(tool_dir)\n",
    "        # print(\"tool_usage\", tool_usages)\n",
    "\n",
    "\n",
    "    return tool_usages\n",
    "\n",
    "\n",
    "# Example \n",
    "response = \"\"\"\n",
    "<tool$tool=\"search\" query=\"example query\">This is a search tool request.</tool$>\n",
    "<tool$tool=\"image\" query=\"example image\">This is an image tool request.</tool$>\n",
    "\"\"\"\n",
    "\n",
    "tool_requests = extract_tool_requests(response)\n",
    "tool_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Alice\n",
      "welcome to Wonderland\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys\n",
    "\n",
    "def read_file(relative_path, **kwargs):\n",
    "    absolute_path = get_abs_path(relative_path)  # Construct the absolute path to the target file\n",
    "    with open(absolute_path) as f:\n",
    "        content = f.read()\n",
    "    # Replace placeholders with values from kwargs\n",
    "    for key, value in kwargs.items():\n",
    "        placeholder = \"{{\" + key + \"}}\"\n",
    "        strval = str(value)\n",
    "        content = content.replace(placeholder, strval)\n",
    "    return content \n",
    "\n",
    "def get_abs_path(*relative_paths):\n",
    "    return os.path.join(get_base_dir(), *relative_paths)\n",
    "\n",
    "def get_base_dir():\n",
    "    # Use the current working directory as the base directory\n",
    "    base_dir = os.getcwd()\n",
    "    return base_dir\n",
    "\n",
    "# Example usage\n",
    "result = read_file('file.txt', name='Alice', place='Wonderland')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys \n",
    "\n",
    "def read_file(relative_path, **kwargs):\n",
    "\n",
    "    absolute_path = get_abs_path(relative_path)\n",
    "    with open(absolute_path) as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Replace placeholder with values from kwargs \n",
    "    for key, value in kwargs.items():\n",
    "        placeholder = \"{{\" + key + \"}}\"\n",
    "        strval = str(value)\n",
    "        content = content.replace(placeholder, strval)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def get_abs_path(*relative_path):\n",
    "    return os.path.join(get_base_dir(), *relative_path)\n",
    "\n",
    "\n",
    "\n",
    "def get_base_dir():\n",
    "\n",
    "    # Use the current working directory as the base directory \n",
    "    base_dir = os.getcwd()\n",
    "    return base_dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m\u001b[38;2;255;0;0m\u001b[48;2;255;255;0mHello world\u001b[0m\n",
      "\u001b[1m\u001b[38;2;255;0;0m\u001b[48;2;255;255;0mHello world@\u001b[0m"
     ]
    }
   ],
   "source": [
    "import webcolors\n",
    "\n",
    "class PrintStyle:\n",
    "    last_endline=True\n",
    "    \n",
    "    def __init__(self, bold=False, italic=False, underline=False, font_color=\"default\", background_color=\"default\", padding=False):\n",
    "        self.bold = bold\n",
    "        self.italic = italic\n",
    "        self.underline = underline\n",
    "        self.font_color = font_color\n",
    "        self.background_color = background_color\n",
    "        self.padding = padding\n",
    "        self.padding_added = False  # Flag to track if padding was added\n",
    "\n",
    "\n",
    "\n",
    "    def _get_rgb_color_code(self, color, is_background=False):\n",
    "        try:\n",
    "            if color.startswith(\"#\") and len(color) == 7:\n",
    "                # Convert hex color to RGB\n",
    "                r = int(color[1:3], 16)\n",
    "                g = int(color[3:5], 16)\n",
    "                b = int(color[5:7], 16)\n",
    "            else:\n",
    "                # Convert named color to RGB\n",
    "                rgb_color = webcolors.name_to_rgb(color)\n",
    "                r, g, b = rgb_color.red, rgb_color.green, rgb_color.blue\n",
    "            \n",
    "            if is_background:\n",
    "                return f\"\\033[48;2;{r};{g};{b}m\"\n",
    "            else:\n",
    "                return f\"\\033[38;2;{r};{g};{b}m\"\n",
    "        except ValueError:\n",
    "            # Fallback to default color\n",
    "            return \"\\033[49m\" if is_background else \"\\033[39m\"\n",
    "        \n",
    "\n",
    "\n",
    "    def _get_styled_text(self, text):\n",
    "        start = \"\"\n",
    "        end = \"\\033[0m\"  # Reset ANSI code\n",
    "        if self.bold:\n",
    "            start += \"\\033[1m\"\n",
    "        if self.italic:\n",
    "            start += \"\\033[3m\"\n",
    "        if self.underline:\n",
    "            start += \"\\033[4m\"\n",
    "        start += self._get_rgb_color_code(self.font_color)\n",
    "        start += self._get_rgb_color_code(self.background_color,True)\n",
    "        return start + text + end\n",
    "    \n",
    "\n",
    "\n",
    "    def _add_padding_if_needed(self):\n",
    "        if self.padding and not self.padding_added:\n",
    "            print()  # Print an empty line for padding\n",
    "            if not PrintStyle.last_endline: print() # add one more if last print was streamed\n",
    "            self.padding_added = True\n",
    "\n",
    "\n",
    "            \n",
    "    def get(self, *args, sep=' ', **kwargs):\n",
    "        text = sep.join(map(str, args))\n",
    "        return self._get_styled_text(text)\n",
    "    \n",
    "    \n",
    "        \n",
    "    def print(self, *args, sep=' ', **kwargs):\n",
    "        self._add_padding_if_needed()\n",
    "        styled_text = self.get(*args, sep=sep, **kwargs)\n",
    "        print(styled_text, end='\\n', flush=True)\n",
    "        PrintStyle.last_endline = True\n",
    "\n",
    "\n",
    "\n",
    "    def stream(self, *args, sep=' ', **kwargs):\n",
    "        self._add_padding_if_needed()\n",
    "        styled_text = self.get(*args, sep=sep, **kwargs)\n",
    "        print(styled_text, end='', flush=True)\n",
    "        PrintStyle.last_endline = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage \n",
    "style = PrintStyle(bold=True, font_color=\"red\", background_color=\"yellow\", padding=True)\n",
    "style.print(\"Hello world\")\n",
    "style.stream(\"Hello world@\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Limiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 1 processed.\n",
      "Request 2 processed.\n",
      "Request 3 processed.\n",
      "Request 4 processed.\n",
      "Request 5 processed.\n",
      "\n",
      "\n",
      "\u001b[38;2;255;255;0m\u001b[49mRate limiter: sleeping for 60.0 seconds...\u001b[0m\n",
      "Request 6 processed.\n",
      "Request 7 processed.\n",
      "Request 8 processed.\n",
      "Request 9 processed.\n",
      "Request 10 processed.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "def rate_limiter(max_requests_per_minute,\n",
    "                 max_tokens_per_mintire):\n",
    "    \n",
    "\n",
    "    execution_times = deque()\n",
    "    token_counts = deque()\n",
    "\n",
    "    def limit(tokens):\n",
    "        if tokens > max_tokens_per_mintire:\n",
    "            raise ValueError(\"Number of tokens execeds the maximum allowed per minute.\")\n",
    "        current_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "        # cleanup old execution times and token counts \n",
    "        while execution_times and current_time - execution_times[0] > 60:\n",
    "            execution_times.popleft()\n",
    "            token_counts.popleft()\n",
    "        total_tokens = sum(token_counts)\n",
    "\n",
    "\n",
    "\n",
    "        if len(execution_times) < max_requests_per_minute and total_tokens + tokens <= max_tokens_per_mintire:\n",
    "            execution_times.append(current_time)\n",
    "            token_counts.append(tokens)\n",
    "\n",
    "\n",
    "        else:\n",
    "            sleep_time = max(\n",
    "                60 - (current_time - execution_times[0]),\n",
    "                60 - (current_time - execution_times[0]) if total_tokens + tokens > max_requests_per_minute else 0 \n",
    "            )\n",
    "\n",
    "\n",
    "            PrintStyle(font_color=\"yellow\", padding=True).print(f\"Rate limiter: sleeping for {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "            current_time = time.time()\n",
    "            execution_times.append(current_time)\n",
    "            token_counts.append(tokens)\n",
    "\n",
    "    return limit\n",
    "\n",
    "\n",
    "\n",
    "# Example usage \n",
    "rate_limit = rate_limiter(5, 10)  # Allow max 5 requests and 10 tokens per minute \n",
    "# rate_limit\n",
    "\n",
    "\n",
    "# Simulate making requests \n",
    "for i in range(10):\n",
    "    try:\n",
    "        rate_limit(2)   # each request uses 2 tokens \n",
    "        print(f\"Request {i + 1} processed.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vectorDB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m vector_db \u001b[38;5;241m=\u001b[39m VectorDB(embeddings_model\u001b[38;5;241m=\u001b[39membeddings_model, in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Insert a document\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m \u001b[43mvector_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_document\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThis is the first sentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 61\u001b[0m, in \u001b[0;36mVectorDB.insert_document\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     59\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings_model\u001b[38;5;241m.\u001b[39mencode([data])\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Add the embeddings to your vector store\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:287\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    286\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:508\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 508\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain\\embeddings\\cache.py:133\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m missing_indices \u001b[38;5;129;01min\u001b[39;00m batch_iterate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, all_missing_indices):\n\u001b[0;32m    132\u001b[0m     missing_texts \u001b[38;5;241m=\u001b[39m [texts[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m missing_indices]\n\u001b[1;32m--> 133\u001b[0m     missing_vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munderlying_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(missing_texts)\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_embedding_store\u001b[38;5;241m.\u001b[39mmset(\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(missing_texts, missing_vectors))\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, updated_vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(missing_indices, missing_vectors):\n",
      "File \u001b[1;32mc:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "from langchain.storage import InMemoryByteStore, LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "import uuid \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class VectorDB:\n",
    "\n",
    "    def __init__(self, embeddings_model, in_memory=False, cache_dir=\"./cache\"):\n",
    "        print(\"Initializing vectorDB...\")\n",
    "        self.embeddings_model = embeddings_model\n",
    "        em_cache = get_abs_path(cache_dir, \"embeddings\")\n",
    "        db_cache = get_abs_path(cache_dir, \"database\")\n",
    "\n",
    "        if in_memory:\n",
    "            self.store = InMemoryByteStore()\n",
    "        else:\n",
    "            self.store = LocalFileStore(em_cache)\n",
    "\n",
    "        # Setup the embeddings model with the chosen cache storage \n",
    "        self.embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "            underlying_embeddings=embeddings_model,\n",
    "            document_embedding_cache=self.store,\n",
    "            namespace=getattr(embeddings_model, 'model', getattr(embeddings_model, 'model_name', \"default\"))\n",
    "        )\n",
    "\n",
    "        self.db = Chroma(embedding_function=self.embedder, persist_directory=db_cache)\n",
    "\n",
    "    def search_similarity(self, query, results=3):\n",
    "        return self.db.similarity_search(query, results)\n",
    "\n",
    "    def search_max_rel(self, query, results=3):\n",
    "        return self.db.max_marginal_relevance_search(query, results)\n",
    "\n",
    "    def delete_documents(self, query):\n",
    "        score_limit = 1\n",
    "        k = 2 \n",
    "        tot = 0 \n",
    "        while True:\n",
    "            # Perform similarity search with score \n",
    "            docs = self.db.similarity_search_with_score(query, k=k)\n",
    "            # Extract document IDs and filter based on score \n",
    "            document_ids = [result[0].metadata[\"id\"] for result in docs if result[1] < score_limit]\n",
    "            # Delete documents with IDs over the threshold score \n",
    "            if document_ids:\n",
    "                fnd = self.db.get(where={\"id\": {\"$in\": document_ids}})\n",
    "                if fnd[\"ids\"]:\n",
    "                    self.db.delete(ids=fnd[\"ids\"])\n",
    "                    tot += len(fnd[\"ids\"])\n",
    "                # If fewer than k document IDs, break the loop \n",
    "                if len(document_ids) < k:\n",
    "                    break\n",
    "        return tot \n",
    "\n",
    "    def insert_document(self, data):\n",
    "        id = str(uuid.uuid4())\n",
    "        # Generate embeddings using the encode method\n",
    "        embeddings = self.embeddings_model.encode([data])\n",
    "        # Add the embeddings to your vector store\n",
    "        self.db.add_documents(documents=[Document(page_content=data, metadata={\"id\": id, \"embeddings\": embeddings.tolist()})])\n",
    "        return id\n",
    "\n",
    "# Example usage \n",
    "embeddings_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize the vectorDB \n",
    "vector_db = VectorDB(embeddings_model=embeddings_model, in_memory=True)\n",
    "\n",
    "# Insert a document\n",
    "doc_id = vector_db.insert_document(\"This is the first sentence\")\n",
    "print(f\"Document ID: {doc_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code Executation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, contextlib\n",
    "from io import StringIO\n",
    "import ast \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_code_with_return_and_function(code):\n",
    "\n",
    "    # Parse the code into a AST \n",
    "    parsed_code = ast.parse(code)\n",
    "\n",
    "    # filter out only executable statements, ignoring comments and empty lines \n",
    "    executable_statements = [stmt for stmt in parsed_code.body if not isinstance(stmt, ast.Pass)]\n",
    "\n",
    "    if not executable_statements:\n",
    "        raise Exception(\"There are no executable statments in the code.\")\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # GEt the last executable statment in the code \n",
    "        last_statement = executable_statements[-1]\n",
    "\n",
    "        # check if the last statement is an expression (including function calls)\n",
    "        if isinstance(last_statement, ast.Expr):\n",
    "\n",
    "            # Convert the expression into a return statement \n",
    "            return_stmt = ast.Return(value=last_statement.value)\n",
    "            return_stmt.lineno = last_statement.lineno\n",
    "            return_stmt.col_offset = last_statement.col_offset\n",
    "            parsed_code.body[parsed_code.body.index(last_statement)] = return_stmt\n",
    "\n",
    "\n",
    "        # wrap the entire code in a function definition \n",
    "        function_def = ast.FunctionDef(\n",
    "            name=\"isolate\",\n",
    "            args=ast.arguments(\n",
    "                posonlyargs=[], args=[], kwonlyargs=[], defaults=[] \n",
    "            ),\n",
    "            body=parsed_code.body,\n",
    "            decorator_list=[],\n",
    "            lineno=1,\n",
    "            col_offset=0\n",
    "\n",
    "        )    # type: ignore \n",
    "\n",
    "    # Create a new module with the function definition \n",
    "    module = ast.Module(body=[function_def],\n",
    "                        type_ignores=[])\n",
    "    \n",
    "\n",
    "    # convert the AST back to source code \n",
    "    wrapped_code = compile(module, filename=\"<ast>\", mode=\"exec\")\n",
    "\n",
    "    return wrapped_code\n",
    "\n",
    "\n",
    "\n",
    "# Example usage \n",
    "\n",
    "code = \"\"\" \n",
    "\n",
    "a = 4 \n",
    "b = 5\n",
    "print(a + b)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "wrapped_code = wrap_code_with_return_and_function(code)\n",
    "# execute wrapped code \n",
    "exec(wrapped_code)\n",
    "\n",
    "# call the function \n",
    "# result = isolate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'get_abs_path' is not defined\n"
     ]
    }
   ],
   "source": [
    "def execute_user_code(code):\n",
    "    try:\n",
    "        wrapped_code = wrap_code_with_return_and_function(code)\n",
    "        exec_globals = {}\n",
    "        exec_locals = {}\n",
    "        exec(wrapped_code, exec_globals, exec_locals)\n",
    "\n",
    "        os.chdir(get_abs_path(\"./work_dir\"))   # change cwd(change working directory) to work dir \n",
    "        try:\n",
    "            return exec_locals.get('isolate', lambda: None)()\n",
    "        \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_info = traceback.format_exc()\n",
    "            return json.dumps({\"error\": str(e),\n",
    "                               \"details\": error_info})\n",
    "        \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"Error: \" + str(e)\n",
    "    \n",
    "\n",
    "# Example usage \n",
    "code = \"\"\" \n",
    "\n",
    "a = 4 \n",
    "b = 5\n",
    "print(a + b)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = execute_user_code(code)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module(\n",
      "    body=[\n",
      "        FunctionDef(\n",
      "            name='hell_world',\n",
      "            args=arguments(\n",
      "                posonlyargs=[],\n",
      "                args=[],\n",
      "                kwonlyargs=[],\n",
      "                kw_defaults=[],\n",
      "                defaults=[]),\n",
      "            body=[\n",
      "                Expr(\n",
      "                    value=Call(\n",
      "                        func=Name(id='print', ctx=Load()),\n",
      "                        args=[\n",
      "                            Constant(value='hello world')],\n",
      "                        keywords=[]))],\n",
      "            decorator_list=[],\n",
      "            type_params=[])],\n",
      "    type_ignores=[])\n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "\n",
    "code = \"\"\" \n",
    "\n",
    "def hell_world():\n",
    "    print(\"hello world\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "parsed_code = ast.parse(code)\n",
    "# print(parsed_code)\n",
    "print(ast.dump(parsed_code, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "5\n",
      "[4, 9, 16, 25, 36]\n",
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "add_ten = lambda x: x + 4 \n",
    "print(add_ten(4))\n",
    "\n",
    "add_numbers = lambda x, y: x + y \n",
    "print(add_numbers(2, 3))\n",
    "\n",
    "numbers = [2, 3, 4, 5, 6]\n",
    "squared_numbers = list(map(lambda x: x ** 2, numbers))\n",
    "print(squared_numbers)\n",
    "\n",
    "# using lambda with filter to get even numbers \n",
    "even_numbers = list(filter(lambda x: x % 2 == 0, numbers))\n",
    "print(even_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, contextlib, time, importlib, inspect\n",
    "from io import StringIO\n",
    "from typing import Optional, Dict\n",
    "\n",
    "from langchain.schema import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    paused = False \n",
    "    streaming_agent = None \n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def configure(model_chat, model_embedding, memory_subdir = \"\", memory_results = 3):\n",
    "\n",
    "        # save configuration \n",
    "        Agent.model_chat = model_chat\n",
    "\n",
    "        # initialize memory tool \n",
    "\n",
    "        from tools import memory_tool\n",
    "        memory_tool.initialize(\n",
    "            embeddings_model=model_embedding,\n",
    "            messages_returned=memory_results,\n",
    "            subdir=memory_subdir\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, \n",
    "                 system_prompt: Optional[str] = None,\n",
    "                 tools_prompt: Optional[str] = None,\n",
    "                 superjior: Optional['Agent'] = None,\n",
    "                 number = 0\n",
    "                 ):\n",
    "        \n",
    "        self.number = number\n",
    "        self.name = f\"Agent {self.number}\"\n",
    "        if system_prompt is None: tools_prompt = read_file(\"./prompts/agent.system.md\")\n",
    "        if tools_prompt is None: tools_prompt = read_file(\"./prompts/agent.tools.md\")\n",
    "\n",
    "        self.system_prompt = system_prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        self.tools_prompt = tools_prompt.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "\n",
    "        self.superior = Optional['Agent'] = None \n",
    "\n",
    "        self.history = []\n",
    "        self.last_message = \"\"\n",
    "        self.message_for_superior = \"\"\n",
    "        self.intervention_message = \"\"\n",
    "        self.intervention_status = False \n",
    "\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.system_prompt + \"\\n\\n\" + self.tools_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\")\n",
    "        ])\n",
    "\n",
    "\n",
    "\n",
    "    def process_message(self, msg: str):\n",
    "\n",
    "        try:\n",
    "            self.append_message(msg, human=True)  # Append the user's input to the history \n",
    "            printer = PrintStyle(italic=True, font_color=\"#b3ffd9\", padding=False)\n",
    "\n",
    "\n",
    "            while True:   # let the agent iterate on his thoughts untill be stops by using a tool \n",
    "                Agent.streaming_agent = self   # mark self as current streamer \n",
    "                agent_response = \"\"\n",
    "                self.intervention_status = False   # reset intervention status \n",
    "\n",
    "                try:\n",
    "\n",
    "                    inputs = {\"input\": msg, \"messages\": self.history}\n",
    "                    chain = self.prompt | Agent.model_chat\n",
    "                    formatted_inputs = self.prompt.format(**inputs)\n",
    "\n",
    "                    # wait for rate limiter - A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughtly % of a word ( so 100 tokens ~= 75 words. )\n",
    "                \n",
    "                    rate_limit(len(formatted_inputs) / 4)  \n",
    "\n",
    "                    # output that the agent is starting \n",
    "                    PrintStyle(bold=True, font_color=\"green\", padding=True, background_color=\"white\").print(f\"{self.name}: Starting a message: \")\n",
    "\n",
    "                    for chunk in chain.stream(inputs):\n",
    "                        \n",
    "                        if self.handle_intervention(agent_response):\n",
    "                            break   # wait for intervention and handle it if paused.\n",
    "\n",
    "\n",
    "                        if chunk.content is not None and chunk.content != '':\n",
    "                            printer.stream(chunk.content)  # output the agent response steam \n",
    "\n",
    "                            agent_response += chunk.content \n",
    "\n",
    "\n",
    "                    if not self.handle_intervention(agent_response):\n",
    "\n",
    "                        # if assist_response is the same as last messge in history, let him know \n",
    "                        if self.last_message == agent_response:\n",
    "                            agent_response = read_file(\"./prompts/fw.msg_repeat.md\")\n",
    "                            PrintStyle(font_color=\"orange\", padding=True).print(agent_response)\n",
    "\n",
    "                        self.last_message = agent_response\n",
    "\n",
    "                        self.append_message(agent_response)\n",
    "                        self.process_message(agent_response)\n",
    "\n",
    "\n",
    "                        # break the execution if there is a message for superior agent \n",
    "                        if self.message_for_superior and not self.intervention_status:\n",
    "                            msg = self.message_for_superior\n",
    "                            self.message_for_superior = \"\"\n",
    "                            return msg \n",
    "                        \n",
    "\n",
    "                # forward errors to the LLm, maybe he can fix them \n",
    "                except Exception as e:\n",
    "                    msg_response = read_file(\"./prompts/fw.error.md\", error=str(e))\n",
    "                    self.append_message(msg_response, human=True)\n",
    "                    PrintStyle(font_color=\"red\", padding=True).print(msg_response)\n",
    "\n",
    "\n",
    "        finally:\n",
    "            Agent.streaming_agent = None    # unset current streamer \n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_message(self, msg: str, human: bool = False):\n",
    "\n",
    "    message_type = \"human\" if human else \"ai\"\n",
    "    if self.history and self.history[-1].type == message_type:\n",
    "        self.history[-1].content += \"\\n\\n\" + msg \n",
    "\n",
    "\n",
    "    else:\n",
    "        new_message = HumanMessage(content=msg) if human else AIMessage(content=msg)\n",
    "        self.history.append(new_message)\n",
    "        self.cleanup_history(5, 10)\n",
    "\n",
    "    if message_type == \"ai\":\n",
    "        self.last_message = msg \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_history(self, x, y):\n",
    "    if len(self.history) <= x + y:\n",
    "        return self.history \n",
    "    \n",
    "\n",
    "    first_x = self.history[:x]\n",
    "    last_y = self.histyr[-y:]\n",
    "\n",
    "    cleanup_prompt = read_file(\"./prompts/fw.msg_cleanup.md\")\n",
    "    middle_values = [AIMessage(content=cleanup_prompt)]\n",
    "\n",
    "    self.history = first_x + middle_values + last_y\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_intervention(self, progress: str = \"\") -> bool:\n",
    "\n",
    "    while self.paused: time.sleep(0.1)  # wait if paused \n",
    "\n",
    "    # if there is an intervention message, but not yet processed \n",
    "    if self.intervention_message and not self.intervention_status:\n",
    "        if progress.strip(): \n",
    "            self.append_message(progress)\n",
    "\n",
    "        # format the user intervention template \n",
    "        user_msg = read_file(\"./prompts/fw.intervention.md\", user_message= self.intervention_message)\n",
    "        self.append_message(user_msg, human=True)\n",
    "        self.intervention_messgae = \"\"\n",
    "        self.intervention_status = True\n",
    "\n",
    "\n",
    "    return self.intervention_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tools(self, msg: str):\n",
    "\n",
    "    # search for tool usage requests in agent message \n",
    "    tool_requests = extract_tool_requests(msg)\n",
    "\n",
    "    for tool_request in tool_requests:\n",
    "        if self.handle_intervention():\n",
    "            break;\n",
    "\n",
    "\n",
    "        tool_name = tool_requests[\"name\"]\n",
    "        tool_function = self.get_tool(tool_name)\n",
    "\n",
    "\n",
    "        if callable(tool_function):\n",
    "\n",
    "            short_params = {k: v for k, v in tool_request.item() if k != \"name\" and k != \"body\"}  # only extra parameters to output to console \n",
    "            PrintStyle(font_color=\"#1B4F72\", padding=True, background_color=\"white\", bold=True).print(f\"{self.name}: Using tool {tool_name}: \")\n",
    "            PrintStyle(font_color=\"#85C1E9\").print(short_params, tool_request['body'], sep=\"\\n\") if short_params else PrintStyle(font_color=\"85C1E9\").print(tool_request[\"body\"])\n",
    "\n",
    "            tool_response = tool_function(self, tool_request[\"body\"], **tool_request) or \"\" # call tool function with all parameters, body parameters separated for convenience \n",
    "            Agent.streaming_agent = self   # mark self as current streamer again, it may have changed during tool use \n",
    "\n",
    "            if self.handle_intervention(): \n",
    "                break\n",
    "\n",
    "            msg_response = read_file(\"./prompts/fw.tool_response.md\", tool_name=tool_name, tool_request=tool_request)\n",
    "            self.append_message(msg_response, human=True)\n",
    "\n",
    "            PrintStyle(font_color=\"#1B4F72\", background_color=\"white\", padding=True, bold=True).print(f\"{self.name}: Response from {tool_name}: \")\n",
    "            PrintStyle(font_color=\"#85C1E9\").print(tool_request)\n",
    "\n",
    "\n",
    "        else:\n",
    "            if self.handle_intervention():\n",
    "                break;\n",
    "\n",
    "            msg_response = read_file(\"./prompts/fw.tool_not_found.md\", tool_name=tool_name, tools_prompt=self.tools_prompt)\n",
    "            self.append_message(msg_response, True)\n",
    "            PrintStyle(font_color=\"orange\", padding=True).print(msg_response)\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool(self, name:str):\n",
    "    module = importlib.import_module(\"tools.\" + name) # Import the module \n",
    "    function_list = {name: func for name, func in inspect.getmembers(module, inspect.isfunction)}  # get all function in the module \n",
    "\n",
    "    if \"execute\" in function_list: \n",
    "        return function_list[\"execute\"]  # check if the module contains a function named \"execute\"\n",
    "    \n",
    "    if function_list:\n",
    "        return next(iter(function_list.values()))  # Return the first function if no \"execute\" function is found \n",
    "    \n",
    "    return None   # Return None if no function are found \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI, OpenAI, OpenAIEmbeddings\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import os\n",
    "\n",
    "DEFAULT_TEMPERATURE = 0.0\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# utility functions to get API keys from environment variables\n",
    "def get_api_key(service):\n",
    "    return os.getenv(f\"API_key_{service.upper()}\")\n",
    "\n",
    "\n",
    "get_api_key(\"openai\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_gpt35(api_key=None, temperature=DEFAULT_TEMPERATURE):\n",
    "    api_key = api_key or get_api_key(\"openai\")\n",
    "    return ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=temperature, api_key=api_key)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for the text: [[0.019295422360301018, 0.01642497256398201, 0.07273902744054794, 0.03308621421456337, 0.059720929712057114, 0.06992031633853912, 0.03729814291000366, -0.005818242207169533, 0.03765908628702164, -0.047984614968299866, 0.045444898307323456, -0.03459855541586876, 0.04935990646481514, 0.00853636022657156, 0.014929299242794514, 0.06183437630534172, 0.08225728571414948, 0.02568216808140278, -0.0424724705517292, -0.019732292741537094, -0.009228910319507122, 0.03725957125425339, 0.08623967319726944, -0.05810059234499931, -0.004230275750160217, -0.0016023291973397136, -0.019658543169498444, 0.06552005559206009, 0.11449294537305832, 0.001924963085912168, 0.033311571925878525, -0.009164861403405666, 0.015894128009676933, 0.04008613899350166, 0.05279970169067383, 0.054821159690618515, -0.010900660417973995, 0.04938409477472305, -0.026740295812487602, 0.02009173482656479, 0.04378341883420944, 0.005599518306553364, 0.010202351026237011, 0.05376637354493141, 0.005870104301720858, -0.08066004514694214, -0.0365082249045372, -0.032877251505851746, -0.04390735924243927, 0.052118074148893356, -0.07428882271051407, -0.06137552484869957, -0.10955638438463211, -0.05561252310872078, -0.014295628294348717, 0.01053643599152565, -0.03156699612736702, -0.016322897747159004, 0.040429193526506424, -0.034785330295562744, 0.017486972734332085, -0.03574458509683609, -0.008917618542909622, 0.08319127559661865, 0.08791622519493103, -0.026916760951280594, -0.017204519361257553, 0.0688236728310585, -0.06091739237308502, 0.07501935213804245, -0.002035729354247451, 0.04913408309221268, -0.08339657634496689, -0.024132193997502327, -0.09744447469711304, -0.032147862017154694, -0.006087370216846466, -0.11881016194820404, 0.14845101535320282, -0.030039113014936447, -0.01827995665371418, -0.0492728129029274, -0.02808031439781189, 0.00968900416046381, 0.05512496083974838, 4.6400360588449985e-05, 0.049912601709365845, -0.13948363065719604, -0.06384868174791336, 0.012537047266960144, -0.053602445870637894, -0.053342197090387344, 0.039767034351825714, 0.04328257218003273, -0.039618585258722305, -0.018756123259663582, -0.06454985588788986, -0.006726385094225407, 0.024893034249544144, 0.090787373483181, -0.022772151976823807, 0.03339853882789612, 0.09074680507183075, 0.03486614301800728, -0.06472120434045792, -0.13890330493450165, -0.04597245156764984, -0.0279831662774086, -0.058953944593667984, -0.056009382009506226, -0.06034281477332115, -0.0380091518163681, -0.027602897956967354, 0.006680632941424847, -0.0024804100394248962, -0.03401773050427437, -0.006284372415393591, -0.02574300579726696, 0.0011851206654682755, 0.013892412185668945, 0.03644467517733574, 0.10309357941150665, -0.09329716116189957, 0.017749352380633354, 0.004068938549607992, -0.11521215736865997, 0.07826685160398483, -4.635754092325653e-33, 0.016756946220993996, -0.025403190404176712, 0.01309364102780819, 0.05410294607281685, 0.04806585609912872, -0.0028863311745226383, -0.0817808136343956, 0.062414705753326416, -0.09138622134923935, 0.01878759078681469, -0.01650058850646019, -0.0378589890897274, -0.014312835410237312, 0.08000448346138, -0.0033102778252214193, 0.04238397255539894, -0.0482662171125412, 0.06607192754745483, -0.01223903801292181, 0.02966303750872612, -0.03910117968916893, 0.04504115879535675, -0.07103445380926132, -0.07570723444223404, -0.08000091463327408, -0.06416730582714081, 0.060636091977357864, -0.06156674027442932, -0.07004337012767792, 0.017827806994318962, -0.026528438553214073, -0.021789152175188065, -0.026721280068159103, -0.041785117238759995, 0.06124943494796753, 0.028559818863868713, 0.06864838302135468, -0.009372108615934849, -0.013347267173230648, 0.023812200874090195, 0.026723245158791542, -0.023685909807682037, 0.04223133623600006, -0.031497806310653687, 0.03147928789258003, 0.03244924917817116, 0.045709218829870224, 0.022051190957427025, -0.033061202615499496, -0.010265599004924297, 0.02041676640510559, 0.020055001601576805, 0.03165007382631302, -0.08247314393520355, 0.08763142675161362, -0.012810989283025265, -0.06128540635108948, 0.06695342063903809, 0.023249201476573944, -0.021896805614233017, -0.06283670663833618, -0.028221867978572845, -0.011288871057331562, 0.01731693744659424, 0.013232356868684292, 0.006974160671234131, -0.002592106582596898, -0.07533437758684158, 0.07456763088703156, 0.037904947996139526, -0.055845655500888824, 0.026267964392900467, -0.08001793920993805, 0.049385685473680496, -0.09842490404844284, 0.010525230318307877, -0.048679742962121964, -0.03724846988916397, 0.015121729113161564, 0.06249609217047691, -0.049937278032302856, -0.12037070840597153, -0.003918303642421961, -0.019844109192490578, -0.016718195751309395, -0.04470311850309372, 0.050918735563755035, -0.16028784215450287, -0.0012021685251966119, 0.07364226877689362, -0.005938760936260223, -0.012111080810427666, -0.027512479573488235, -0.015823552384972572, 0.04862510412931442, 1.336509250729605e-33, 0.0013126820558682084, 0.026958949863910675, -0.04142148047685623, 0.08708897978067398, 0.06970804929733276, 0.031131599098443985, 0.0013976471964269876, 0.0008394762990064919, 0.01637742854654789, 0.08132444322109222, -0.11678149551153183, -0.012145450338721275, 0.05324757844209671, -0.06345507502555847, -0.03346747159957886, 0.053935009986162186, 0.014158140867948532, -0.001873754314146936, -0.02174217440187931, 0.047258052974939346, -0.01076930109411478, 0.07264392077922821, 0.009601837024092674, 0.11749914288520813, 0.05533444508910179, 0.09865810722112656, 0.01855679415166378, -0.01812247931957245, -0.07207255065441132, -0.03566810116171837, -0.00963599793612957, -0.1312655359506607, -0.08060815185308456, -0.007500502280890942, -0.055472616106271744, 0.03295351564884186, 0.13858714699745178, -0.04205057770013809, -0.05152041092514992, -0.020764611661434174, 0.054519593715667725, 0.008962173946201801, -0.06452654302120209, 0.08936269581317902, 0.03391474112868309, -0.07811347395181656, -0.06303100287914276, -0.09281641989946365, 0.04021826386451721, 0.014966470189392567, -0.1168346107006073, 0.033659521490335464, -0.0759916752576828, -0.027295369654893875, -0.04399802163243294, -0.025270327925682068, 7.760656444588676e-05, -0.07458969205617905, 0.03816796839237213, -0.02374095842242241, -0.0510067418217659, -0.000790187215898186, -0.005076485686004162, -0.05219879373908043, 0.10646118223667145, -0.09775694459676743, -0.0338614284992218, 0.09188235551118851, -0.04655834659934044, -0.02669055014848709, 0.022871065884828568, 0.0073451874777674675, -0.05403892323374748, 0.030530162155628204, -0.01725313812494278, -0.06250119209289551, 0.04431283473968506, -0.0018586840014904737, -0.022559167817234993, -0.055747140198946, 0.03159372881054878, -0.08401250839233398, 0.05513450875878334, 0.01975206658244133, 0.031373031437397, 0.013154431246221066, 0.002638759557157755, -0.02537333033978939, -0.02162528969347477, 0.015406965278089046, -0.022085901349782944, -0.049306176602840424, 0.005534410942345858, 0.0676058828830719, 0.01577983796596527, -2.265149312563608e-08, -0.010097379796206951, 0.013479351066052914, 0.04112911596894264, 0.0027736478950828314, -0.02124825306236744, -0.008058447390794754, 0.015910957008600235, -0.0331890806555748, -0.03993961587548256, -0.006574357859790325, 0.0727757140994072, 0.024734951555728912, -0.048559918999671936, 0.026166753843426704, 0.04389653727412224, 0.050709083676338196, -0.0400390625, -0.0008683297783136368, -0.002229007426649332, 0.06667762249708176, 0.01582162082195282, 0.09233464300632477, 0.006802293937653303, 0.004005208145827055, -0.027859030291438103, 0.00329043110832572, -0.00869149062782526, -0.01989632099866867, 0.0418647825717926, 0.01022530347108841, 0.01580805331468582, 0.12079373002052307, -0.004679679870605469, -0.02149350941181183, 0.0036709073465317488, 0.013453414663672447, 0.027898840606212616, -0.11115071177482605, -0.006513698026537895, -0.07823437452316284, 0.01256512850522995, 0.058495525270700455, -0.03459321707487106, 0.01359531283378601, 0.10535730421543121, 0.0019458666210994124, 0.053255777806043625, -0.0032531165052205324, -4.370550232124515e-05, 0.03332604840397835, 0.013350216671824455, -0.04030685871839523, 0.06095854938030243, 0.0012549072271212935, 0.0395146906375885, 0.015317650511860847, 0.008934582583606243, 0.008523699827492237, -0.03805181011557579, 0.011351730674505234, 0.001977008767426014, 0.08335519582033157, 0.0375698059797287, -0.043819621205329895]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manjusha Kumari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def get_embedding_hf(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    return HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Example usage\n",
    "# Initialize the embedding model\n",
    "embedding_model = get_embedding_hf()\n",
    "\n",
    "# Example text to embed\n",
    "text = \"This is a sample sentence for embedding.\"\n",
    "\n",
    "# Get the embeddings\n",
    "embeddings = embedding_model.embed_documents([text])\n",
    "\n",
    "print(f\"Embeddings for the text: {embeddings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, sys, time, readline\n",
    "from ansio import application_keypad, mouse_input\n",
    "from ansio.input import InputEvent, get_input_event\n",
    "# from tools.helpers.print_style import PrintStyle\n",
    "\n",
    "\n",
    "input_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main conversation loop\n",
    "\n",
    "def chat():\n",
    "\n",
    "    # chat model used for agents \n",
    "    chat_llm = get_openai_gpt35()\n",
    "\n",
    "    # embedding model used for memory \n",
    "    embedding_llm = get_embedding_hf()\n",
    "\n",
    "    # initial configuration \n",
    "    Agent.configure(\n",
    "        model_chat=chat_llm,\n",
    "        model_embedding=\n",
    "        embedding_llm\n",
    "    )\n",
    "\n",
    "\n",
    "    # create the first agent \n",
    "    agent0 = Agent()\n",
    "\n",
    "\n",
    "    # start the conversation loop \n",
    "    while True:\n",
    "\n",
    "        # ask user for message \n",
    "        PrintStyle(background_color=\"#6C3483\", font_color=\"white\", bold=True, padding=True).print(f\"User message ('exit' to leave): \")\n",
    "\n",
    "        with input_lock:\n",
    "            user_input = input(\"> \").strip()\n",
    "\n",
    "        # exit the conversation when the user types 'exit'\n",
    "        if user_input.lower() == 'exit':\n",
    "            break \n",
    "\n",
    "\n",
    "        # send message to agent0 \n",
    "        assistant_response = agent0.process_message(user_input)\n",
    "\n",
    "\n",
    "        # print agent0 response \n",
    "        PrintStyle(font_color=\"white\", background_color=\"#1D8348\", bold=True, padding=True).print(f\"{agent0.name}: response: \")\n",
    "        PrintStyle(font_color=\"white\").print(f\"{assistant_response}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user intervention during agent streaming \n",
    "def intervention():\n",
    "\n",
    "    if Agent.streaming_agent and not Agent.paused:\n",
    "        Agent.paused = True   # stop agent streaming \n",
    "\n",
    "        PrintStyle(background_color=\"#6C3483\", font_color=\"white\", bold=True, padding=True).print(f\"User intervention ('exit' to leave, empty to continoue) :\")\n",
    "\n",
    "        user_input = input(\"> \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            sys.exit()  # exit the conversation when the user types 'exit' \n",
    "\n",
    "        if user_input:\n",
    "            Agent.streaming_agent.intervention_message = user_input   # set intervention message if non-empty \n",
    "\n",
    "        Agent.paused = False   # continue agent streaming \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture keybord input to trigger user intervention \n",
    "def capture_keys():\n",
    "\n",
    "    global input_lock\n",
    "    intervent = False \n",
    "\n",
    "    while True:\n",
    "        if intervent:\n",
    "            intervention()\n",
    "\n",
    "        intervent = False\n",
    "\n",
    "        if Agent.streaming_agent:\n",
    "            with input_lock, application_keypad, mouse_input:\n",
    "                event: InputEvent | None = get_input_event(timeout=0.1)\n",
    "                if event and (event.shortcut.isalpha() or event.shortcut.isspace()):\n",
    "                    intervent = True\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing framework....\")\n",
    "\n",
    "    # start the key capture thread for user intervention during agent streaming \n",
    "    threading.Thread(target=capture_keys, daemon=True).start()\n",
    "\n",
    "    # start the chat \n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
